{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed41f47-8c56-47c1-a2d1-ef37b973be41",
   "metadata": {},
   "source": [
    "# 07 Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9f949-c3c2-4248-b7ae-d011d7a53739",
   "metadata": {},
   "source": [
    "In the classical setup, we have a null hypothesis,$H_0$ , that represents some default position, and some alternative hypothesis, $H_1$, that we’d like to compare it with. We use statistics to decide whether we can reject $H_0$ as false or not. This will probably make more sense with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b323a2-ea0b-4150-8cdc-6a51d53c689e",
   "metadata": {},
   "source": [
    "### Example: Flipping a Coin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9dbff-cfcf-4213-bdec-92a0654de769",
   "metadata": {},
   "source": [
    "Imagine we have a coin and we want to test whether it’s fair. We’ll make the assumption that the coin has some probability p of landing heads, and so our null hypothesis is that the coin is fair—that is, that p = 0.5. We’ll test this against the alternative hypothesis p ≠ 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd01e1-17b1-4588-9182-405d3fde578a",
   "metadata": {},
   "source": [
    "In particular, our test will involve flipping the coin some number, n, times and counting the number of heads, X. Each coin flip is a Bernoulli trial, which means that X is a Binomial(n,p) random variable which we can approximate using the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baebd500-30e2-4752-adea-9390b4510172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from typing import List\n",
    "\n",
    "import math\n",
    "\n",
    "def normal_approximation_to_binmomial(n:int, p:float) -> Tuple[float, float]:\n",
    "    \"\"\"Returns mu and sigma corresponding to a Binomial(n, p)\"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c7667-ddf1-4cf2-8188-e92b81909301",
   "metadata": {},
   "source": [
    "Whenever a random variable follows a normal distribution, we can use\n",
    "`normal_cdf` to figure out the probability that its realized value lies within\n",
    "or outside a particular interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2162d3-4215-4546-abdf-34b5e2c535ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratchlib.probability import normal_cdf\n",
    "\n",
    "# the normal cdf _is_ the probability the variable is below\n",
    "# a threshold\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# its above the threhsold if its not belowe the threshold\n",
    "def normal_probability_above(lo:float,\n",
    "                             mu:float=0,\n",
    "                             sigma:float=1) -> float:\n",
    "    \"\"\"The probability that an N(mu, sigma) is greater than lo.\"\"\"\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# its between if its less than hi, but not less than lo\n",
    "def normal_probability_between(lo:float,\n",
    "                               hi:float,\n",
    "                               mu:float=0,\n",
    "                               sigma:float=1) -> float:\n",
    "    \"\"\"The probability that an N(mu, sigma) is between lo and hi.\"\"\"\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# its outside iff its not between\n",
    "def normal_probability_outside(lo:float,\n",
    "                               hi:float,\n",
    "                               mu:float=0,\n",
    "                               sigma:float=1) -> float:\n",
    "    \"\"\"The probability that an N(mu, sigma) is not between lo and hi.\"\"\"\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b83d77-c3b7-4569-8adb-11ed5a7b7e22",
   "metadata": {},
   "source": [
    "We can also do the reverse—find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood. For example, if we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd2f5aa-b2bc-43b2-9026-3d10bf01b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratchlib.probability import inverse_normal_cdf\n",
    "\n",
    "def normal_upper_bound(probability:float,\n",
    "                       mu:float=0,\n",
    "                       sigma:float=1) -> float:\n",
    "    \"\"\"Returns the z for which P(Z <= z) = probability.\"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability:float,\n",
    "                       mu:float=0,\n",
    "                       sigma:float=1) -> float:\n",
    "    \"\"\"Returns the z for which P(Z >= z) = probability.\"\"\"\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability:float,\n",
    "                            mu:float=0,\n",
    "                            sigma:float=1) -> Tuple[float, float]:\n",
    "    \"\"\"Returns the symmetric (about the mean) bounds\n",
    "    that contains the specified probability\n",
    "    \"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    # upper bound should have tail_probability above it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    # lower bound should have tail_ptobability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a495cd83-d1d1-40ec-a111-ac385716eaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0 15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binmomial(1000, 0.5)\n",
    "print(mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29a8f4-88d1-4e7c-a7cd-bafa2c7366ee",
   "metadata": {},
   "source": [
    "We need to make a decision about __significance__ —how willing we are to make a __type 1 error (“false positive”)__, in which we reject $H_0$ even though it’s true. For reasons lost to the annals of history, this willingness is often set at 5% or 1%. Let’s choose 5%.\n",
    "\n",
    "Consider the test that rejects $H_0$ if X falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55f4191-63c6-492f-a6ff-a3e63688325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469.01026640487555 530.9897335951244\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a14ba-f8a7-4ef8-8358-7171eee0ab93",
   "metadata": {},
   "source": [
    "Assuming p really equals 0.5 (i.e., $H_0$ is true), there is just a 5% chance we observe an $X$ that lies outside this interval, which is the exact significance we wanted. Said differently, if $H_0$ is true, then, approximately 19 times out of 20, this test will give the correct result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90d5cb-69d7-4953-8fdc-10104cb7cf10",
   "metadata": {},
   "source": [
    "We are also often interested in the __power__ of a test, which is the probability of __not__ making a __type 2 error (“false negative”)__, in which we fail to reject $H_0$\n",
    "even though it’s false. In order to measure this, we have to specify what exactly being false means. (Knowing merely that p is not 0.5 doesn’t give us a ton of information about the distribution of X.) In particular, let’s check what happens if p is really 0.55, so that the coin is slightly biased toward heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdaa93-1cac-4a4c-a87b-0d3e4bc2bbae",
   "metadata": {},
   "source": [
    "In that case, we can calculate the power of the test with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44cd08e-a9b7-4ee9-b506-256467760bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo: 469.01026640487555,  hi: 530.9897335951244\n",
      "mu_1: 550.0,  sigma_1: 15.732132722552274\n",
      "type II prob: 0.11345199870463285,  power: 0.8865480012953671\n"
     ]
    }
   ],
   "source": [
    "# 95% bounds based on assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(f'lo: {lo},  hi: {hi}')\n",
    "\n",
    "# actual mu and sigma absed on p = 0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binmomial(1000, 0.55)\n",
    "print(f'mu_1: {mu_1},  sigma_1: {sigma_1}')\n",
    "\n",
    "# a type 2 error means we fail to reject the null hypothesis,\n",
    "# which will happen when X is still in our original interval\n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "\n",
    "print(f'type II prob: {type_2_probability},  power: {power}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc2034-ca80-4ea2-9686-600b92a6b94d",
   "metadata": {},
   "source": [
    "This is a more powerful test, since it no longer rejects $H_0$ when $X$ is below 469 (which is very unlikely to happen if $H_1$ is true) and instead rejects $H_0$ when X is between 526 and 531 (which is somewhat likely to happen if $H_0$ is true).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15055d43-02c1-4448-bd3b-f68bc072cb0a",
   "metadata": {},
   "source": [
    "## p-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8647bf1-7a62-450e-87b9-61a17ed66d4e",
   "metadata": {},
   "source": [
    "An alternative way of thinking about the preceding test involves p-values. Instead of choosing bounds based on some probability cutoff, we compute the probability—assuming $H_0$ is true—that we would see a value at least as extreme as the one we actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d8f84-155e-4a8d-ab7b-c821589a1d64",
   "metadata": {},
   "source": [
    "For our two-sided test of whether the coin is fair, we compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf9d6ac-ffc1-4ff6-beaa-0620de1f02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_p_value(x:float, mu:float=0, sigma:float=1) -> float:\n",
    "    \"\"\"How likely are we to see a value at least as extreme as x\n",
    "    (in either direction) if our values are from an N(mu, sigma)?\n",
    "    \"\"\"\n",
    "    if x >= mu:\n",
    "        # x is greater than the mean, so the tail is everything greater than x\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        # x is less than the mean, so the tail is everything less than x\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0948644-637f-4464-8ca8-354db7d65bce",
   "metadata": {},
   "source": [
    "If we were to see 530 heads, we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce3edeb-4d99-4c72-8169-a7550c823ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598835"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(529.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daacae3-30a7-40a7-aa33-4550824a6f0f",
   "metadata": {},
   "source": [
    "Why did we use a value of 529.5 rather than using 530? This is what's called a continuity correction. It reflects the fact that `normal_probability_between(529.5, 530.5, mu_0, sigma_0)` is a better estimate of the probability of seeing 530 heads than\n",
    "`normal_probability_between(530, 531, mu_0, sigma_0)` is.\n",
    "\n",
    "Correspondingly, `normal_probability_above(529.5, mu_0, sigma_0)` is a better estimate\n",
    "of the probability of seeing at least 530 heads. You may have noticed that we also used this in the code that produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e2fd4-1a6a-4d8c-90f7-61c2c2611238",
   "metadata": {},
   "source": [
    "One way to convince yourself that this is a sensible estimate is with a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a4744c-1b6d-41c9-8b89-40ed28ddfd35",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9_/jz04jn6j1qz1fn3yjp87z_8c0000gp/T/ipykernel_15233/3610371751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# p-value was 0.062 => ~62 extreme values out of 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m59\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mextreme_value_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{extreme_value_count}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: 71"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "extreme_value_count = 0\n",
    "for __ in range(0, 1000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0\n",
    "                    for __ in range(0, 1000))\n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        extreme_value_count += 1\n",
    "\n",
    "# p-value was 0.062 => ~62 extreme values out of 1000\n",
    "assert 59 < extreme_value_count < 65, f'{extreme_value_count}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c175ec-d5a0-4644-9d65-f48fb63c5675",
   "metadata": {},
   "source": [
    "Since the p-value is greater than our 5% significance, we don’t reject the null. If we instead saw 532 heads, the p-value would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920980ad-9e3d-4808-984b-7ff886425744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30890aaa-39bf-4e03-9440-ddbe7959782d",
   "metadata": {},
   "source": [
    "which is smaller than the 5% significance, which means we would reject the null. It’s the exact same test as before. It’s just a different way of approaching the statistics.\n",
    "Similarly, we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c327675c-9fc1-4242-81bf-e8554c874bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d142e-d9c5-4f29-bd7a-85f04749441a",
   "metadata": {},
   "source": [
    "For our one-sided test, if we saw 525 heads we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d688a7fe-25db-4bd8-9dbe-69b50229e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582072"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(524.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82073bc1-1cfe-4b1f-8f14-dad3b9de3343",
   "metadata": {},
   "source": [
    "which means we wouldn’t reject the null. If we saw 527 heads, the computation would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b9f8c5-8a89-4c67-b6e3-b5ba75ba3ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47559b8-95a5-41ce-8348-1da9bf0fe256",
   "metadata": {},
   "source": [
    "and we would reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b5ca6-938c-45bd-ab48-d5cad1f6dc85",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "We’ve been testing hypotheses about the value of the heads probability p, which is a parameter of the unknown “heads” distribution. When this is the case, a third approach is to construct a confidence interval around the observed value of the parameter.\n",
    "\n",
    "For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip— 1 if heads, 0 if tails. If we observe 525 heads out of 1,000 flips, then we estimate p equals 0.525.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398830bb-3c97-434f-b956-a685e4ccc2c7",
   "metadata": {},
   "source": [
    "How confident can we be about this estimate? Well, if we knew the exact value of p, the central limit theorem (recall “The Central Limit Theorem”) tells us that the average of those Bernoulli variables should be approximately normal, with mean p and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0130db7a-bf59-46ee-98d8-af122f080dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015791611697353755\n"
     ]
    }
   ],
   "source": [
    "# we don't now p, we instead use our estimate\n",
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e72eb4-5d4b-49a9-9d8d-455ad0596cd3",
   "metadata": {},
   "source": [
    "This is not entirely justified, but people seem to do it anyway. Using the normal approximation, we conclude that we are “95% confident” that the following interval contains the true parameter p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dc918e1-b901-44e9-a63a-dec7e53a0477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a974063-11a0-43cf-a721-f4b804ca31ba",
   "metadata": {},
   "source": [
    "__NOTE__\n",
    "\n",
    "This is a statement about the interval, not about p. You should understand it as the assertion that if you were to repeat the experiment many times, 95% of the time the “true” parameter (which is the same every time) would lie within the observed confidence interval (which might be different every time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb26403-d270-48b9-b1ca-501b7d4051cb",
   "metadata": {},
   "source": [
    "In particular, we do not conclude that the coin is unfair, since 0.5 falls within our confidence interval.\n",
    "If instead we’d seen 540 heads, then we’d have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb26a0a4-5496-41f6-abbf-d1d128b97581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat* (1 - p_hat) / 1000)\n",
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433bcf1-f9d4-4e8f-be26-10096c6dc17b",
   "metadata": {},
   "source": [
    "Here, “fair coin” doesn’t lie in the confidence interval. (The “fair coin” hypothesis doesn’t pass a test that you’d expect it to pass 95% of the time if it were true.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc4047-bdf4-46af-8755-f68bf677d46b",
   "metadata": {},
   "source": [
    "## p-Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd094d4f-2323-4998-a98b-a81d0a2a0abe",
   "metadata": {},
   "source": [
    "A procedure that erroneously rejects the null hypothesis only 5% of the time will—by definition—5% of the time erroneously reject the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "242045b3-3f8f-4105-acec-a26b36711278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def run_experiment() -> List[bool]:\n",
    "    \"\"\"Flips a fair coin 1000 times, True = heads, False = tails\"\"\"\n",
    "    return [random.random() < 0.5 for __ in range(0, 1000)]\n",
    "\n",
    "def reject_fairness(experiment:List[bool]) -> bool:\n",
    "    \"\"\"Using the 5% significance levels\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_experiment() for __ in range(0, 1000)]\n",
    "num_rejections = len([experiment for experiment in experiments\n",
    "                                 if reject_fairness(experiment)])\n",
    "\n",
    "assert num_rejections == 46, f'{num_rejections}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17294aff-6c05-408b-bc98-3f9c295ee4a7",
   "metadata": {},
   "source": [
    "What this means is that if you’re setting out to find “significant” results, you usually can. Test enough hypotheses against your dataset, and one of them will almost certainly appear significant. Remove the right outliers, and you can probably get your p-value below 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd7832-0fb3-444e-942c-882cd612e633",
   "metadata": {},
   "source": [
    "This is sometimes called p-hacking and is in some ways a consequence of the “inference from p-values framework.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64379425-0645-4ca8-aab9-120370092eb2",
   "metadata": {},
   "source": [
    "If you want to do good science, you should determine your hypotheses before looking at the data, you should clean your data without the hypotheses in mind, and you should keep in mind that p-values are not substitutes for common sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dcad42-7bd4-438f-838e-1b13fe5b6b8f",
   "metadata": {},
   "source": [
    "## Example: Running an A/B Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a40fd9-d0e4-4bc1-8ef1-a82127fd3196",
   "metadata": {},
   "source": [
    "One of your primary responsibilities at DataSciencester is experience optimization, which is a euphemism for trying to get people to click on advertisements. One of your advertisers has developed a new energy drink targeted at data scientists, and the VP of Advertisements wants your help choosing between advertisement A (“tastes great!”) and advertisement B (“less bias!”).\n",
    "\n",
    "Being a scientist, you decide to run an experiment by randomly showing site visitors one of the two advertisements and tracking how many people click on each one.\n",
    "If 990 out of 1,000 A-viewers click their ad, while only 10 out of 1,000 B- viewers click their ad, you can be pretty confident that A is the better ad. But what if the differences are not so stark? Here’s where you’d use statistical inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27755296-2385-4fe3-b85a-2ab23aa28d3e",
   "metadata": {},
   "source": [
    "Let’s say that $N_A$ people see ad A, and that $n_A$ of them click it. We can think of each ad view as a Bernoulli trial where $p_A$ is the probability that someone clicks ad A. Then (if $N_A$ is large, which it is here) we know that $n_A / N_A$\n",
    "is approximately a normal random variable with mean and standard deviation\n",
    "$ \\sigma_A = \\sqrt{p_A(1 - p_A) / N_A} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3f3b5-b917-46c4-bef2-de24d6269709",
   "metadata": {},
   "source": [
    "Similarly, $n_B / N_B$ is approximately a normal random variable with mean\n",
    "$p_B$ and standard deviation $ \\sigma_B = \\sqrt{p_B (1 - p_B) / N_B} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba55d3-5a71-4b2b-a636-41e754746fd0",
   "metadata": {},
   "source": [
    "in code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5ac0adb-aa9f-4ec7-b99f-8a99819236e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_parameters(N:int, n:int) -> Tuple[float, float]:\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p) / N)\n",
    "    return p, sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f029be-48b8-4415-bc00-76a4dfd92b93",
   "metadata": {},
   "source": [
    "If we assume those two normals are independent (which seems reasonable, since the individual Bernoulli trials ought to be), then their difference should also be normal with mean $p_B - p_A$ and standard deviation $\\sqrt{\\sigma_A ^2 + \\sigma_B ^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ee3a6-6e6d-4ffe-aaeb-9e9b87188ee0",
   "metadata": {},
   "source": [
    "This is sort of cheating. The math only works out exactly like this if you know the standard deviations. Here we’re estimating them from the data, which means that we really should be using a t-distribution. But for large enough datasets, it’s close enough that it doesn’t make much of a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9630fd7-937e-4a17-a74c-1dd26940fcf2",
   "metadata": {},
   "source": [
    "This means we can test the null hypothesis that $p_A$ and $p_B$ are the same (that is, that is 0) by using the statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "74640347-0f17-497d-acbc-a664819cf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_b_test_statistic(N_A:int, n_A:int, N_B:int, n_B:int) -> float:\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876aa980-09b9-42b9-8f4d-b195fc8acbf8",
   "metadata": {},
   "source": [
    "which should approximately be a standard normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0a175-90ea-460d-baac-71db45b9a2ae",
   "metadata": {},
   "source": [
    "For example, if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180 clicks out of 1,000 views, the statistic equals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea9b78cb-1905-4641-88e1-effcec6b6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed75b1-338d-4131-b6d6-15b0b7f102cb",
   "metadata": {},
   "source": [
    "The probability of seeing such a large difference if the means were actually equal would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6948a2d7-42de-4f54-a9b8-67a1481201cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76959692-dd45-4d6c-a0b8-bab054f9ae9f",
   "metadata": {},
   "source": [
    "which is large enough that we can’t conclude there’s much of a difference. On the other hand, if “less bias” only got 150 clicks, we’d have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82a193d5-94fd-4191-b928-43f7dd837fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 150)\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e9c65-ead6-4684-9134-3a8f968650d7",
   "metadata": {},
   "source": [
    "which means there’s only a 0.003 probability we’d see such a large difference if the ads were equally effective.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b75445-dc23-4044-8f44-b79cf0384189",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13dfb2-bef7-4ab3-b4a0-0412787fa6a5",
   "metadata": {},
   "source": [
    "The procedures we’ve looked at have involved making probability statements about our tests: e.g., “There’s only a 3% chance you’d observe such an extreme statistic if our null hypothesis were true.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b454c89-369e-410b-b461-5bd1c1064c38",
   "metadata": {},
   "source": [
    "An alternative approach to inference involves __treating the unknown parameters themselves as random variables__. The analyst (that’s you) starts with a _prior distribution_ for the parameters and then uses the observed data and Bayes’s theorem to get an _updated posterior distribution_ for the parameters. __Rather than making probability judgments about the tests, you make probability judgments about the parameters.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cda77d-8c72-45d3-8361-25d1fc0e3a70",
   "metadata": {},
   "source": [
    "\n",
    "For example, when the unknown parameter is a probability (as in our coin- flipping example), we often use a prior from the Beta distribution, which puts all its probability between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "297b8935-6521-4328-a4f0-1f89a48c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(alpha:float, beta:float) -> float:\n",
    "    \"\"\"A normalizing constant so that the total prob is 1\"\"\"\n",
    "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x:float, alpha:float, beta:float) -> float:\n",
    "    if x <= 0 or x >= 1:\n",
    "        return 0\n",
    "    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dfa21-999f-40ac-a922-26fa43978b61",
   "metadata": {},
   "source": [
    "Generally speaking, this distribution centers its weight at:\n",
    "\n",
    "```python\n",
    "alpha / (alpha + beta)\n",
    "```\n",
    "\n",
    "and the larger and are, the “tighter” the distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e157b89-23c6-4491-bbe6-8b605ffadf45",
   "metadata": {},
   "source": [
    "For example, if and are both 1, it’s just the uniform distribution (centered at 0.5, very dispersed). If is much larger than , most of the weight is near 1. And if is much smaller than , most of the weight is near 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567a8fa-3b92-4785-ab89-4a70ecfb8847",
   "metadata": {},
   "source": [
    "Say we assume a prior distribution on p. Maybe we don’t want to take a stand on whether the coin is fair, and we choose and to both equal 1. Or maybe we have a strong belief that the coin lands heads 55% of the time, and we choose equals 55, equals 45."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166643fe-0a81-44bf-93b8-450d8357ff8b",
   "metadata": {},
   "source": [
    "Then we flip our coin a bunch of times and see h heads and t tails. Bayes’s theorem (and some mathematics too tedious for us to go through here) tells us that the posterior distribution for p is again a Beta distribution, but with parameters and ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e43bd-17db-4ee2-891a-4b44462cbe0b",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "    \n",
    "It is no coincidence that the posterior distribution was again a Beta distribution. The number of heads is given by a Binomial distribution, and the Beta is the conjugate prior to the Binomial distribution. This means that whenever you update a Beta prior using observations from the corresponding binomial, you will get back a Beta posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83e2f2-3acc-4f07-9c78-afb17bb57e54",
   "metadata": {},
   "source": [
    "Let’s say you flip the coin 10 times and see only 3 heads. If you started with the uniform prior (in some sense refusing to take a stand about the coin’s fairness), your posterior distribution would be a Beta(4, 8), centered around 0.33. Since you considered all probabilities equally likely, your best guess is close to the observed probability.\n",
    "If you started with a Beta(20, 20) (expressing a belief that the coin was roughly fair), your posterior distribution would be a Beta(23, 27), centered around 0.46, indicating a revised belief that maybe the coin is slightly biased toward tails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdcb3c1-656d-49b8-a978-39b41f511662",
   "metadata": {},
   "source": [
    "And if you started with a Beta(30, 10) (expressing a belief that the coin was biased to flip 75% heads), your posterior distribution would be a Beta(33, 17), centered around 0.66. In that case you’d still believe in a heads bias, but less strongly than you did initially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833b384-b71a-4391-a21c-fcc927b02940",
   "metadata": {},
   "source": [
    "If you flipped the coin more and more times, the prior would matter less and less until eventually you’d have (nearly) the same posterior distribution no matter which prior you started with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e788cdd-f972-4acb-80b6-48c7e613c507",
   "metadata": {},
   "source": [
    "For example, no matter how biased you initially thought the coin was, it would be hard to maintain that belief after seeing 1,000 heads out of 2,000 flips (unless you are a lunatic who picks something like a Beta(1000000,1) prior)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117825a-bc71-4e79-ae59-aed3921c0d09",
   "metadata": {},
   "source": [
    "What’s interesting is that this allows us to make probability statements about hypotheses: “Based on the prior and the observed data, there is only a 5% likelihood the coin’s heads probability is between 49% and 51%.” This is philosophically very different from a statement like “If the coin were fair, we would expect to observe data so extreme only 5% of the time.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef3fe7-e24a-45f1-8009-0e33b0fe0343",
   "metadata": {},
   "source": [
    "Using Bayesian inference to test hypotheses is considered somewhat controversial—in part because the mathematics can get somewhat complicated, and in part because of the subjective nature of choosing a prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b897456-6821-44ec-9091-4b8b49630a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
