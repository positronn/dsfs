{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd294562-facd-48b3-b9ba-ee362dd2ca75",
   "metadata": {},
   "source": [
    "# 08 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff8079-2684-4866-ab3a-514c594b0d3a",
   "metadata": {},
   "source": [
    "## The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64066b3-56d0-41f3-a5c2-3941620190e0",
   "metadata": {},
   "source": [
    "Suppose we have some function `f` that takes as input a vector of real numbers and outputs a single real number. One simple such function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10716f7c-b254-41f8-83dd-09ac01883460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratchlib.linalg import Vector\n",
    "from scratchlib.linalg import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934594d2-272f-4992-b34b-a15361c09e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v:Vector) -> float:\n",
    "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a30697-8a21-43df-897f-9fd3e38806db",
   "metadata": {},
   "source": [
    "We’ll frequently need to maximize or minimize such functions. That is, we need to find the input `v` that produces the largest (or smallest) possible value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad3ed6-1000-4c35-9c63-36de13833d12",
   "metadata": {},
   "source": [
    "For functions like ours, the gradient (if you remember your calculus, this is the vector of partial derivatives) gives the input direction in which the function most quickly increases. (If you don’t remember your calculus, take my word for it or look it up on the internet.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a036921-e566-4107-9a50-c57c25041657",
   "metadata": {},
   "source": [
    "Accordingly, one approach to maximizing a function is to pick a random starting point, compute the gradient, take a small step in the direction of the gradient (i.e., the direction that causes the function to increase the most), and repeat with the new starting point. Similarly, you can try to minimize a function by taking small steps in the opposite direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbdcc5-5def-429a-9994-7d358ed50dc7",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "If a function has a unique global minimum, this procedure is likely to find it. If a function has multiple (local) minima, this procedure might “find” the wrong one of them, in which case you might rerun the procedure from different starting points. If a function has no minimum, then it’s possible the procedure might go on forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fed51-02a5-43e2-952a-b16f39b872db",
   "metadata": {},
   "source": [
    "## Estimating the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b551cef-0d7d-4cfe-b9c2-fcdeb4ffebf3",
   "metadata": {},
   "source": [
    "If `f` is a function of one vriable, its derivative at a point `x` measures how `f(x)` changes when we make a very small change to `x`. The derivative is defined as the limit of the difference quotients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b1dc01-91f6-4ae4-bd4b-57581103aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f:Callable[[float], float],\n",
    "                        x:float,\n",
    "                        h:float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce503c5-553e-491e-96f6-980002164023",
   "metadata": {},
   "source": [
    "as `h` approaches zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473a7ea-5603-42f5-9019-206e2fe1d04c",
   "metadata": {},
   "source": [
    "The derivative is the slope of the tangent line at $(x, f(x))$ , while the difference quotient is the slope of the not-quite-tangent line that runs through $(x + h, f(x + h)$. As h gets smaller and smaller, the not-quite- tangent line gets closer and closer to the tangent line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124e8ff-cd7a-4c94-83fb-604716d9cb1d",
   "metadata": {},
   "source": [
    "For many functions it’s easy to exactly calculate derivatives. For example, the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202bf41b-8049-4a0e-a3bd-ebd9ee9d5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x:float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b02fa-fe5f-4c7f-bafe-96af2b43eb62",
   "metadata": {},
   "source": [
    "has the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15310322-1e08-4a90-b83d-232a74d91c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_derivative(x:float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dfa5d-2904-4c15-8ea0-a5bca1846fd2",
   "metadata": {},
   "source": [
    "which is easy for us to check by explicitly computing the difference quotient and taking the limit. (Doing so requires nothing more than high school algebra.)\n",
    "What if you couldn’t (or didn’t want to) find the gradient? Although we can’t take limits in Python, we can estimate derivatives by evaluating the difference quotient for a very small `e`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952969a6-b94a-42df-9868-7822377345dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = range(-10, 11)\n",
    "actuals = [square_derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "\n",
    "# plot to show they're basically the same\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Actual derivatives vs. estimates')\n",
    "plt.plot(xs, actuals, 'rx', label='Actual')\n",
    "plt.plot(xs, estimates, 'b+', label='Estimate')\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9920a-3e2c-4911-99bc-12407c7b040d",
   "metadata": {},
   "source": [
    "When `f` is a function of many variables, it has multiple partial derivatives, each indicating how changes when we make small changes in just one of the input variables.\n",
    "We calculate its ith partial derivative by treating it as a function of just its\n",
    "ith variable, holding the other variables fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b7c797-9f08-4df1-b63d-30fae54d9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f:Callable[[Vector], float],\n",
    "                                v:Vector,\n",
    "                                i:int,\n",
    "                                h:float) -> float:\n",
    "    \"\"\"Returns the i-th partial difference quotient of `f` at `v`\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "             for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40ee7d-45f6-4774-8bb2-2632f2822c56",
   "metadata": {},
   "source": [
    "after which we can estimate the gradient the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "224755ac-09a5-4e80-8a39-0060881914d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f:Callable[[Vector], float],\n",
    "                      v:Vector,\n",
    "                      h:float=0.0001) -> Vector:\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "                for i in range(0, len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c12ea-a776-42fb-b8e3-17b2b9847fb7",
   "metadata": {},
   "source": [
    "## Using the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f9fea-2145-4d4a-940e-5ddbbc264abd",
   "metadata": {},
   "source": [
    "It’s easy to see that `sum_of_squares` the function is smallest when its input `v` is a vector of zeros. But imagine we didn’t know that. Let’s use gradients\n",
    "to find the minimum among all three-dimensional vectors. We’ll just pick a random starting point and then take tiny steps in the opposite direction of the gradient until we reach a point where the gradient is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9ed864-7c00-4fb3-86ca-ec7a2164ea3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [6.630549480248547, -9.106965543491205, 8.711599119614343]\n",
      "100 [0.879340527408749, -1.2077617259266227, 1.155328406376377]\n",
      "200 [0.11661775022520633, -0.16017282371906935, 0.153219140166228]\n",
      "300 [0.015465794244312119, -0.021242048748031567, 0.02031985432342111]\n",
      "400 [0.0020510667642403037, -0.002817111071258643, 0.0026948100562181865]\n",
      "500 [0.00027201156338403756, -0.00037360401917651344, 0.00035738451287637075]\n",
      "600 [3.607405273422857e-05, -4.9547199103684066e-05, 4.7396175381327e-05]\n",
      "700 [4.784124852937301e-06, -6.570927540959498e-06, 6.2856597301814235e-06]\n",
      "800 [6.344685133415959e-07, -8.714334922986511e-07, 8.336014019221947e-07]\n",
      "900 [8.414293246856683e-08, -1.1556912274046102e-07, 1.105518477161652e-07]\n",
      "1000 [1.1158998335663543e-08, -1.532672573298616e-08, 1.4661336947462222e-08]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scratchlib.linalg import distance\n",
    "from scratchlib.linalg import add\n",
    "from scratchlib.linalg import scalar_multiply\n",
    "\n",
    "def gradient_step(v:Vector, gradient:Vector, step_size:float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v:Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(0, 3)]\n",
    "\n",
    "for epoch in range(0, 1001):\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    v = gradient_step(v, grad, -0.01)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "assert distance(v, [0, 0, 0]) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c0c5f-d788-4803-bd38-83ab327c14dd",
   "metadata": {},
   "source": [
    "you’ll find that it always ends up with a `v` that’s very close to `[0, 0, 0]`. The more epochs you run it for, the closer it will get.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44181821-25cc-4dec-a2f5-3a2316c4c8f5",
   "metadata": {},
   "source": [
    "## Choosing the Right Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8f8dc-be64-4fb7-a09e-ebb5c869d5c0",
   "metadata": {},
   "source": [
    "Although the rationale for moving against the gradient is clear, how far to move is not. Indeed, choosing the right step size is more of an art than a science. Popular options include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949b288-b8c0-4cc8-9f8b-13903b389b9e",
   "metadata": {},
   "source": [
    "- Using a fixed step size\n",
    "- Gradually shrinking the step size over time\n",
    "- At each step, choosing the step size that minimizes the value of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f742769-dc25-448a-baa9-3805ab245951",
   "metadata": {},
   "source": [
    "The last approach sounds great but is, in practice, a costly computation. To keep things simple, we’ll mostly just use a fixed step size. The step size that “works” depends on the problem—too small, and your gradient descent will\n",
    "take forever; too big, and you’ll take giant steps that might make the function you care about get larger or even be undefined. So we’ll need to experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54869a2-f5e5-4a53-aed9-15e8f610d6b0",
   "metadata": {},
   "source": [
    "## Using Gradient Descent to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be58348-3ba1-4680-8196-0d4411614336",
   "metadata": {},
   "source": [
    "In this book, we’ll be using gradient descent to fit parameterized models to data. In the usual case, we’ll have some dataset and some (hypothesized) model for the data that depends (in a differentiable way) on one or more parameters. We’ll also have a loss function that measures how well the model fits our data. (Smaller is better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbadbf-c3c0-4853-a705-ecb77cae1abe",
   "metadata": {},
   "source": [
    "If we think of our data as being fixed, then our loss function tells us how good or bad any particular model parameters are. This means we can use gradient descent to find the model parameters that make the loss as small as possible. Let’s look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4827e2ec-88d9-4625-9392-a379c6a76702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc3589-9093-449b-b7d9-99adc42cd6c4",
   "metadata": {},
   "source": [
    "In this case we know the parameters of the linear relationship between `x` and `y`, but imagine we’d like to learn them from the data. We’ll use gradient\n",
    "descent to find the slope and intercept that minimize the average squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f3de0-e0ef-4324-aeae-f14b244c433f",
   "metadata": {},
   "source": [
    "We’ll start off with a function that determines the gradient based on the error from a single data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3688987d-cb8f-4529-aedf-336b8056e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x:float, y:float, theta:Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept\n",
    "    error = (predicted - y)\n",
    "    squared_error = error ** 2\n",
    "    grad = [2 * error * x, 2 * error]\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a21dc4-1a02-403f-a40e-d3cac1aa21b2",
   "metadata": {},
   "source": [
    "Let’s think about what that gradient means. Imagine for some `x` our prediction is too large. In that case the `error` is positive. The second\n",
    "gradient term, `2 * error` , is positive, which reflects the fact that small increases in the intercept will make the (already too large) prediction even larger, which will cause the squared error (for this `x` ) to get even bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8dbbd-edc2-4409-9b26-580365401b32",
   "metadata": {},
   "source": [
    "The first gradient term, `2 * error * x`, has the same sign as `x`. Sure enough, if `x` is positive, small increases in the slope will again make the\n",
    "prediction (and hence the error) larger. If `x` is negative, though, small increases in the slope will make the prediction (and hence the error) smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ea6f6-7908-48fb-bb18-823f6ed44963",
   "metadata": {},
   "source": [
    "\n",
    "Now, that computation was for a single data point. For the whole dataset we’ll look at the mean squared error. And the gradient of the mean squared error is just the mean of the individual gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f353a-9d6e-468d-8611-b8843c56bc0f",
   "metadata": {},
   "source": [
    "So, here’s what we’re going to do:\n",
    "1. Start with a random value for .\n",
    "2. Compute the mean of the gradients.\n",
    "3. Adjust in that direction.\n",
    "4. Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c0bb7-94f9-4773-bf98-93c529376a84",
   "metadata": {},
   "source": [
    "After a lot of epochs (what we call each pass through the dataset), we should learn something like the correct parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae32c489-7abd-49e5-8a06-6a001820974a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [33.213806143205886, -0.058880234949821204]\n",
      "200 [19.997967088215724, 1.6152006582105816]\n",
      "400 [19.99863768021838, 2.7317367453903323]\n",
      "600 [19.999087065557028, 3.4799636041371773]\n",
      "800 [19.99938821317255, 3.9813745648561216]\n",
      "1000 [19.999590021906698, 4.317386228417853]\n",
      "1200 [19.999725260451115, 4.542558486095736]\n",
      "1400 [19.999815888163404, 4.693453681489576]\n",
      "1600 [19.99987662071765, 4.794573420784979]\n",
      "1800 [19.999917319561877, 4.862337020868347]\n",
      "2000 [19.999944593170603, 4.907747595779386]\n",
      "2200 [19.99996287009584, 4.9381786872682385]\n",
      "2400 [19.999975118053168, 4.9585715435801765]\n",
      "2600 [19.99998332580457, 4.972237454601197]\n",
      "2800 [19.999988826083626, 4.981395422527695]\n",
      "3000 [19.999992511998094, 4.987532472330943]\n",
      "3200 [19.999994982048314, 4.991645107425304]\n",
      "3400 [19.999996637308662, 4.994401116902437]\n",
      "3600 [19.99999774655203, 4.996248007779884]\n",
      "3800 [19.99999848989181, 4.997485668949586]\n",
      "4000 [19.999998988027773, 4.998315065634419]\n",
      "4200 [19.999999321844758, 4.998870871114668]\n",
      "4400 [19.99999954554629, 4.999243334300888]\n",
      "4600 [19.999999695455905, 4.999492933900061]\n",
      "4800 [19.999999795915176, 4.999660198645172]\n",
      "5000 [19.999999863236173, 4.999772288147923]\n"
     ]
    }
   ],
   "source": [
    "from scratchlib.linalg import vector_mean\n",
    "\n",
    "# start with random values for slipe and intercept\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(0, 5001):\n",
    "    # compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # take a step in that direction\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1, 'slope should be about 20'\n",
    "assert 4.9 < intercept < 5.1, 'intercept should be about 5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df44e0-83d0-491a-9c2f-339ba0df2a18",
   "metadata": {},
   "source": [
    "## Minibatch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fff103-9a25-496e-a061-52f3c8136a62",
   "metadata": {},
   "source": [
    "One drawback of the preceding approach is that we had to evaluate the gradients on the entire dataset before we could take a gradient step and update our parameters. In this case it was fine, because our dataset was only 100 pairs and the gradient computation was cheap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b12608-de9d-4d1b-b330-d32f3f91733d",
   "metadata": {},
   "source": [
    "Your models, however, will frequently have large datasets and expensive gradient computations. In that case you’ll want to take gradient steps more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b25387-0056-4195-95e0-aa4b438c2da5",
   "metadata": {},
   "source": [
    "We can do this using a technique called minibatch gradient descent, in which we compute the gradient (and take a gradient step) based on a “minibatch” sampled from the larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc20c4b-5ffb-42eb-8f6a-da87abf82249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def minibatches(dataset:List[T],\n",
    "                batch_size:int,\n",
    "                shuffle:bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    if shuffle:\n",
    "        random.shuffle(batch_starts)\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed4044-7b1a-41da-9e1a-4d848fb71327",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "The allows us to create a “generic” function. It says that our can be a list of any single type— s, s, s, whatever—but whatever that type is, the outputs will be batches of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621709e-fc07-45d7-a804-2d09821a89eb",
   "metadata": {},
   "source": [
    "Now we can solve our problem again using minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a1761c-8dcd-4908-a42d-de126cf78e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [18.28474712551498, 1.0233655076541717]\n",
      "200 [20.017402685171927, 4.907276024947338]\n",
      "400 [19.998496026392612, 4.993498942947153]\n",
      "600 [20.00009940942343, 4.999764432579968]\n",
      "800 [19.999997991838093, 4.99998738744728]\n",
      "1000 [20.00000001478466, 4.999999368436632]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(0, 1001):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1, 'slope should be about 20'\n",
    "assert 4.9 < intercept < 5.1, 'intercept should be about 5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747d0df-a158-4842-8ef8-6a34aa2138a4",
   "metadata": {},
   "source": [
    "Another variation is stochastic gradient descent, in which you take gradient steps based on one training example at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43776c14-ec74-4c7a-868c-f6199ab8b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [20.113720173074928, -0.6600941184296731]\n",
      "20 [20.04730936336546, 2.6452986822136735]\n",
      "40 [20.019681558357547, 4.020401747673814]\n",
      "60 [20.008187856966963, 4.592469444640535]\n",
      "80 [20.00340630610282, 4.830459933161123]\n",
      "100 [20.001417075380804, 4.929468270633139]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(0, 101):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 20 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1, 'slope should be about 20'\n",
    "assert 4.9 < intercept < 5.1, 'intercept should be about 5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161b61c-1827-4b77-9483-b57585fdc8b5",
   "metadata": {},
   "source": [
    "On this problem, stochastic gradient descent finds the optimal parameters in a much smaller number of epochs. But there are always tradeoffs. Basing gradient steps on small minibatches (or on single data points) allows you to take more of them, but the gradient for a single point might lie in a very different direction from the gradient for the dataset as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4de47-cd45-4353-b06c-806929949562",
   "metadata": {},
   "source": [
    "In addition, if we weren’t doing our linear algebra from scratch, there would be performance gains from “vectorizing” our computations across batches rather than computing the gradient one point at a time.\n",
    "Throughout the book, we’ll play around to find optimal batch sizes and step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f8830-e5d9-4865-b64b-ae590a275ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
